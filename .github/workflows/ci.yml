name: Build

on:
  push:
    branches:
      - main
  pull_request_target:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      PRINTERS_JS_SIMULATE: true # Force simulation mode for all tests
      CARGO_TERM_COLOR: always
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      # Install system dependencies for Linux printing
      - name: Install CUPS development libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      # Install cargo-llvm-cov for code coverage
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      # Install cargo2junit for JUnit XML output from Cargo tests
      - name: Install cargo2junit
        run: cargo install cargo2junit

      # Cache Rust dependencies for faster builds
      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.
          cache: true

      # Setup Node.js for N-API module and testing
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      # Setup Bun for cross-runtime testing
      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Code quality checks
      - name: Check Rust formatting
        run: cargo fmt --check

      - name: Run Rust linter (Clippy)
        run: cargo clippy -- -D warnings

      - name: Check Deno formatting
        run: deno task fmt

      - name: Run Deno linter
        run: deno task lint

      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      # Build all runtime libraries using our cross-platform TypeScript script
      - name: Build all runtime libraries
        run: deno run --allow-run --allow-read --allow-env scripts/build-all.ts

      # Run Cargo tests with coverage (Rust-level tests)
      - name: Run Cargo tests with coverage
        run: |
          mkdir -p test-results/coverage
          cargo llvm-cov --all-features --workspace --lcov --output-path test-results/coverage/rust.lcov test

      # Run Cargo tests again for JUnit report (separate from coverage run)
      - name: Generate Cargo test JUnit report
        run: |
          mkdir -p test-results
          # Run cargo tests with JSON output and convert to JUnit
          cargo test --all-features --workspace -- -Z unstable-options --format json --report-time 2>/dev/null | cargo2junit > test-results/cargo.xml || true
          
          # Fallback: If cargo2junit fails, run tests normally and create a simple JUnit file
          if [ ! -s test-results/cargo.xml ]; then
            echo "cargo2junit failed, generating fallback JUnit XML..."
            cargo test --all-features --workspace > cargo_test_output.txt 2>&1
            TEST_COUNT=$(grep -c "test result:" cargo_test_output.txt || echo "0")
            PASSED=$(grep "test result: ok" cargo_test_output.txt | grep -oE "[0-9]+ passed" | grep -oE "[0-9]+" || echo "0")
            FAILED=$(grep "test result: FAILED" cargo_test_output.txt | grep -oE "[0-9]+ failed" | grep -oE "[0-9]+" || echo "0")
            
            echo '<?xml version="1.0" encoding="UTF-8"?>' > test-results/cargo.xml
            echo '<testsuites>' >> test-results/cargo.xml
            echo "  <testsuite name=\"Rust Cargo Tests\" tests=\"${PASSED}\" failures=\"${FAILED}\" errors=\"0\">" >> test-results/cargo.xml
            echo '    <testcase name="Cargo test suite" classname="cargo" />' >> test-results/cargo.xml
            echo '  </testsuite>' >> test-results/cargo.xml
            echo '</testsuites>' >> test-results/cargo.xml
          fi

      # Run comprehensive cross-runtime tests using our TypeScript script
      - name: Run cross-runtime tests
        run: deno run --allow-run --allow-write --allow-read --allow-env scripts/test-all.ts

      # Debug: List generated test files
      - name: Debug - List test results
        if: always()
        run: |
          echo "=== Test Results Directory ==="
          ls -la test-results/ || echo "test-results directory not found"
          echo ""
          echo "=== XML Files Content Preview ==="
          for file in test-results/*.xml; do
            if [ -f "$file" ]; then
              echo "--- $file ---"
              head -20 "$file"
              echo ""
            fi
          done

      - name: Upload Test Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: test-reports
          path: test-results/*.xml

      - name: Upload Coverage Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: test-results/coverage/*

  report:
    if: ${{ always() && github.event_name == 'pull_request_target' }}
    runs-on: ubuntu-latest
    needs: build
    permissions:
      contents: read
      pull-requests: write
      actions: read
      checks: write
    steps:
      - name: Checkout merge commit
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.

      - uses: actions/download-artifact@v5

      # - name: Convert JUnit XML to CTRF
      #   run: |
      #     mkdir -p test-results/ctrf
      #     deno run -A npm:junit-to-ctrf "test-reports/*.xml" -o test-results/ctrf/junit.json

      # - name: Publish Cross-Runtime Test Report
      #   uses: ctrf-io/github-test-reporter@v1
      #   with:
      #     title: Cross-Runtime Printer Library Test Report
      #     report-path: "./test-results/ctrf/*.json"
      #     pull-request-report: true
      #     flaky-rate-report: true
      #     pull-request: true
      #     overwrite-comment: true
      #     status-check: true
      #     status-check-name: "Cross-Runtime Tests (Deno/Bun/Node.js)"
      #     annotate: true
      #   env:
      #     GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup LCOV
        uses: hrishikesh-kadam/setup-lcov@v1

      # Create combined coverage report from all runtimes
      - name: Combine coverage reports
        run: |
          echo "Combining coverage reports from all runtimes..."
          cd coverage-reports

          # Create individual reports and combined report
          if [ -f rust.lcov ]; then
            cp rust.lcov combined.lcov
          else
            touch combined.lcov
          fi

          if [ -f deno-lcov.info ]; then
            cat deno-lcov.info >> combined.lcov
          fi

          # Note: Bun coverage is in different format, handled separately
          echo "Coverage files available:"
          ls -la

          # Clean up the combined coverage file to remove problematic entries
          # This filters out TLA:GNC lines which can cause genhtml errors
          if [ -f combined.lcov ]; then
            echo "Cleaning combined coverage file..."
            grep -v "TLA:GNC" combined.lcov > combined_clean.lcov || true
            mv combined_clean.lcov combined.lcov
          fi

      # Report coverage for each runtime separately
      # - name: Report Rust code coverage
      #   if: ${{ hashFiles('coverage-reports/rust.lcov') != '' }}
      #   uses: zgosalvez/github-actions-report-lcov@v4
      #   with:
      #     title-prefix: "🦀 [Rust Core]"
      #     coverage-files: coverage-reports/rust.lcov
      #     artifact-name: rust-coverage-report
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     update-comment: true

      # - name: Report Deno code coverage
      #   if: ${{ hashFiles('coverage-reports/deno-lcov.info') != '' }}
      #   uses: zgosalvez/github-actions-report-lcov@v4
      #   with:
      #     title-prefix: "🦕 [Deno FFI]"
      #     coverage-files: coverage-reports/deno-lcov.info
      #     artifact-name: deno-coverage-report
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     update-comment: true

      # - name: Report Combined coverage
      #   if: ${{ hashFiles('coverage-reports/combined.lcov') != '' }}
      #   uses: zgosalvez/github-actions-report-lcov@v4
      #   with:
      #     title-prefix: "📊 [Combined Coverage]"
      #     coverage-files: coverage-reports/combined.lcov
      #     artifact-name: combined-coverage-report
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     update-comment: true

      # Add/update dynamic test summary comment based on actual results
      - name: Update Test Summary Comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Function to parse JUnit XML (handles different formats)
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              // Pattern 1: Standard JUnit with tests, errors, failures attributes
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);
              
              // Pattern 2: Alternative format with different attribute order
              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);
                
                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return {
                    total: tests,
                    errors: errors,
                    failures: failures,
                    passed: tests - errors - failures
                  };
                }
              }
              
              // Pattern 3: Count individual testcase elements
              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;
                
                if (testcases > 0) {
                  return {
                    total: testcases,
                    errors: errors,
                    failures: failures,
                    passed: testcases - errors - failures
                  };
                }
              }
              
              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }
              
              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Function to format test result
            function formatTestResult(result) {
              const total = result.passed + result.failures + result.errors;
              const status = result.failures === 0 && result.errors === 0 ? '✅' : '❌';
              return { status, text: `${result.passed}/${total} passed` };
            }

            // Generate the summary content
            let summary = `## 🧪 Cross-Runtime Test Results\n\n`;
            summary += `This PR was tested across **3 JavaScript runtimes**:\n\n`;
            summary += `| Runtime | Status | Tests | Coverage |\n`;
            summary += `|---------|--------|-------|----------|\n`;

            // Read Deno test results
            let denoResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/deno-test-results.xml')) {
                const denoXml = fs.readFileSync('test-reports/deno-test-results.xml', 'utf8');
                console.log('Deno XML preview:', denoXml.substring(0, 500));
                const parsed = parseJUnit(denoXml);
                console.log('Deno parsed result:', parsed);
                denoResult = formatTestResult(parsed);
              } else {
                console.log('Deno test results file not found');
              }
            } catch (e) {
              console.log('Error reading Deno results:', e.message);
            }

            // Read Rust/Cargo test results
            let rustResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/cargo.xml')) {
                const cargoXml = fs.readFileSync('test-reports/cargo.xml', 'utf8');
                const parsed = parseJUnit(cargoXml);
                rustResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Rust results:', e.message);
            }

            // Read Bun test results
            let bunResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/bun-test-results.xml')) {
                const bunXml = fs.readFileSync('test-reports/bun-test-results.xml', 'utf8');
                const parsed = parseJUnit(bunXml);
                bunResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Bun results:', e.message);
            }

            // Read Node.js test results
            let nodeResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/node-test-results.xml')) {
                const nodeXml = fs.readFileSync('test-reports/node-test-results.xml', 'utf8');
                const parsed = parseJUnit(nodeXml);
                nodeResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Node.js results:', e.message);
            }

            // Check for coverage files
            const denoCoverage = fs.existsSync('coverage-reports/deno-lcov.info') ? 'LCOV available' : 'No coverage';
            const rustCoverage = fs.existsSync('coverage-reports/rust.lcov') ? 'LCOV available' : 'No coverage';
            const bunCoverage = fs.existsSync('coverage-reports/bun-lcov.info') ? 'LCOV available' : 'No coverage';
            const nodeCoverage = fs.existsSync('coverage-reports/node-lcov.info') ? 'LCOV available' : 'No coverage';

            summary += `| 🦕 **Deno** | ${denoResult.status} FFI | ${denoResult.text} | ${denoCoverage} |\n`;
            summary += `| 🦀 **Rust Core** | ${rustResult.status} Native | ${rustResult.text} | ${rustCoverage} |\n`;
            summary += `| 🥟 **Bun** | ${bunResult.status} FFI | ${bunResult.text} | ${bunCoverage} |\n`;
            summary += `| 🟢 **Node.js** | ${nodeResult.status} N-API+tsx | ${nodeResult.text} | ${nodeCoverage} |\n\n`;

            // List available artifacts
            summary += `### 📊 Test Artifacts Generated\n\n`;
            const testFiles = fs.existsSync('test-reports') ? fs.readdirSync('test-reports') : [];
            const coverageFiles = fs.existsSync('coverage-reports') ? fs.readdirSync('coverage-reports') : [];

            if (testFiles.length > 0) {
              summary += `**JUnit Reports**: ${testFiles.filter(f => f.endsWith('.xml')).join(', ')}\n\n`;
            }

            if (coverageFiles.length > 0) {
              summary += `**Coverage Reports**: ${coverageFiles.join(', ')}\n\n`;
            }

            summary += `### 🔧 Architecture\n\n`;
            summary += `- **Universal Entry Point**: \`index.ts\` auto-detects runtime and loads appropriate implementation\n`;
            summary += `- **Deno & Bun**: Use FFI → Rust native library (deno.ts, bun.ts)\n`;
            summary += `- **Node.js**: Uses N-API bindings via TypeScript wrapper (node.ts)\n`;
            summary += `- **Shared Tests**: Single test suite (\`shared.test.ts\`) runs on all runtimes via index.ts\n`;
            summary += `- **Cross-Platform Scripts**: All build/test scripts are Deno TypeScript for consistency\n\n`;

            // Overall status
            const overallSuccess = denoResult.status === '✅' && rustResult.status === '✅' && bunResult.status === '✅' && nodeResult.status === '✅';
            const overallStatus = overallSuccess ? '✅ All tests passing' : '❌ Some tests failed';
            summary += `**Overall Status**: ${overallStatus}\n\n`;

            // Add timestamp and run info
            const timestamp = new Date().toISOString().replace('T', ' ').substring(0, 19);
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            summary += `*Last updated: ${timestamp} UTC | [View CI Run](${runUrl})* 🤖`;

            // Look for existing comment from this bot
            const COMMENT_IDENTIFIER = '## 🧪 Cross-Runtime Test Results';

            try {
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              // Find existing test results comment
              const existingComment = comments.data.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes(COMMENT_IDENTIFIER)
              );

              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: summary
                });
                console.log('Updated existing test results comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created new test results comment');
              }
            } catch (error) {
              console.error('Error managing comment:', error);
              // Fallback: try to create a new comment
              try {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created fallback comment');
              } catch (fallbackError) {
                console.error('Fallback comment creation failed:', fallbackError);
              }
            }
