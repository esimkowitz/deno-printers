name: Build

on:
  push:
    branches:
      - main
  pull_request_target:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      PRINTERS_JS_SIMULATE: true # Force simulation mode for all tests
      CARGO_TERM_COLOR: always
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      # Install system dependencies for Linux printing
      - name: Install CUPS development libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      # Install cargo-llvm-cov for code coverage
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      # Install cargo2junit for JUnit XML output from Cargo tests
      - name: Install cargo2junit
        run: cargo install cargo2junit

      # Cache Rust dependencies for faster builds
      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.
          cache: true

      # Setup Node.js for N-API module and testing
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      # Setup Bun for cross-runtime testing
      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Code quality checks (continue on error to gather all results)
      - name: Check Rust formatting
        id: rust-fmt
        run: cargo fmt --check
        continue-on-error: true

      - name: Run Rust linter (Clippy)
        id: rust-lint
        run: cargo clippy -- -D warnings
        continue-on-error: true

      - name: Check Deno formatting
        id: deno-fmt
        run: deno task fmt
        continue-on-error: true

      - name: Run Deno linter
        id: deno-lint
        run: deno task lint
        continue-on-error: true

      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      - name: Run ESLint for non-Deno files
        id: eslint
        run: npm run lint
        continue-on-error: true

      # Build all runtime libraries using our cross-platform TypeScript script
      - name: Build all runtime libraries
        run: deno task build:all

      # Run Cargo tests with coverage (Rust-level tests)
      - name: Run Cargo tests with coverage
        run: |
          mkdir -p test-results/coverage
          cargo llvm-cov --all-features --workspace --lcov --output-path test-results/coverage/rust.lcov test
        continue-on-error: true

      # Run Cargo tests again for JUnit report (separate from coverage run)
      - name: Generate Cargo test JUnit report
        run: |
          mkdir -p test-results
          # Run cargo tests with JSON output and convert to JUnit
          cargo test --all-features --workspace -- -Z unstable-options --format json --report-time 2>/dev/null | cargo2junit > test-results/cargo.xml || true

          # Fallback: If cargo2junit fails, run tests normally and create a simple JUnit file
          if [ ! -s test-results/cargo.xml ]; then
            echo "cargo2junit failed, generating fallback JUnit XML..."
            cargo test --all-features --workspace > cargo_test_output.txt 2>&1
            TEST_COUNT=$(grep -c "test result:" cargo_test_output.txt || echo "0")
            PASSED=$(grep "test result: ok" cargo_test_output.txt | grep -oE "[0-9]+ passed" | grep -oE "[0-9]+" || echo "0")
            FAILED=$(grep "test result: FAILED" cargo_test_output.txt | grep -oE "[0-9]+ failed" | grep -oE "[0-9]+" || echo "0")

            echo '<?xml version="1.0" encoding="UTF-8"?>' > test-results/cargo.xml
            echo '<testsuites>' >> test-results/cargo.xml
            echo "  <testsuite name=\"Rust Cargo Tests\" tests=\"${PASSED}\" failures=\"${FAILED}\" errors=\"0\">" >> test-results/cargo.xml
            echo '    <testcase name="Cargo test suite" classname="cargo" />' >> test-results/cargo.xml
            echo '  </testsuite>' >> test-results/cargo.xml
            echo '</testsuites>' >> test-results/cargo.xml
          fi
        continue-on-error: true

      # Run comprehensive cross-runtime tests using our TypeScript script
      - name: Run cross-runtime tests
        run: deno task test:all
        continue-on-error: true

      # Debug: List generated test files
      - name: Debug - List test results
        if: ${{ always() }}
        run: |
          echo "=== Test Results Directory ==="
          ls -la test-results/ || echo "test-results directory not found"
          echo ""
          echo "=== XML Files Content Preview ==="
          for file in test-results/*.xml; do
            if [ -f "$file" ]; then
              echo "--- $file ---"
              head -20 "$file"
              echo ""
            fi
          done

      - name: Upload Test Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: test-reports
          path: test-results/*.xml
          if-no-files-found: ignore

      - name: Upload Coverage Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: test-results/coverage/*
          if-no-files-found: ignore

  report:
    if: ${{ always() && github.event_name == 'pull_request_target' }}
    runs-on: ubuntu-latest
    needs: build
    permissions:
      contents: read
      pull-requests: write
      actions: read
      checks: write
    steps:
      - name: Checkout merge commit
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.

      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true

      # - name: Convert JUnit XML to CTRF
      #   run: |
      #     mkdir -p test-results/ctrf
      #     deno run -A npm:junit-to-ctrf "test-reports/*.xml" -o test-results/ctrf/junit.json

      # - name: Publish Cross-Runtime Test Report
      #   uses: ctrf-io/github-test-reporter@v1
      #   with:
      #     title: Cross-Runtime Printer Library Test Report
      #     report-path: "./test-results/ctrf/*.json"
      #     pull-request-report: true
      #     flaky-rate-report: true
      #     pull-request: true
      #     overwrite-comment: true
      #     status-check: true
      #     status-check-name: "Cross-Runtime Tests (Deno/Bun/Node.js)"
      #     annotate: true
      #   env:
      #     GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup LCOV
        uses: hrishikesh-kadam/setup-lcov@v1

      # Create combined coverage report from all runtimes
      - name: Combine coverage reports
        run: |
          echo "Combining coverage reports from all runtimes..."
          cd coverage-reports

          # Create individual reports and combined report
          if [ -f rust.lcov ]; then
            cp rust.lcov combined.lcov
          else
            touch combined.lcov
          fi

          if [ -f deno-lcov.info ]; then
            cat deno-lcov.info >> combined.lcov
          fi

          # Note: Bun coverage is in different format, handled separately
          echo "Coverage files available:"
          ls -la

          # Clean up the combined coverage file to remove problematic entries
          # This filters out TLA:GNC lines which can cause genhtml errors
          if [ -f combined.lcov ]; then
            echo "Cleaning combined coverage file..."
            grep -v "TLA:GNC" combined.lcov > combined_clean.lcov || true
            mv combined_clean.lcov combined.lcov
          fi

      # Report coverage for each runtime separately
      # - name: Report Rust code coverage
      #   if: ${{ hashFiles('coverage-reports/rust.lcov') != '' }}
      #   uses: zgosalvez/github-actions-report-lcov@v4
      #   with:
      #     title-prefix: "🦀 [Rust Core]"
      #     coverage-files: coverage-reports/rust.lcov
      #     artifact-name: rust-coverage-report
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     update-comment: true

      # - name: Report Deno code coverage
      #   if: ${{ hashFiles('coverage-reports/deno-lcov.info') != '' }}
      #   uses: zgosalvez/github-actions-report-lcov@v4
      #   with:
      #     title-prefix: "🦕 [Deno FFI]"
      #     coverage-files: coverage-reports/deno-lcov.info
      #     artifact-name: deno-coverage-report
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     update-comment: true

      # - name: Report Combined coverage
      #   if: ${{ hashFiles('coverage-reports/combined.lcov') != '' }}
      #   uses: zgosalvez/github-actions-report-lcov@v4
      #   with:
      #     title-prefix: "📊 [Combined Coverage]"
      #     coverage-files: coverage-reports/combined.lcov
      #     artifact-name: combined-coverage-report
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     update-comment: true

      # Add/update dynamic test summary comment based on actual results
      - name: Update Test Summary Comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Function to parse JUnit XML (handles different formats)
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              // Pattern 1: Standard JUnit with tests, errors, failures attributes
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              // Pattern 2: Alternative format with different attribute order
              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return {
                    total: tests,
                    errors: errors,
                    failures: failures,
                    passed: tests - errors - failures
                  };
                }
              }

              // Pattern 3: Count individual testcase elements
              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return {
                    total: testcases,
                    errors: errors,
                    failures: failures,
                    passed: testcases - errors - failures
                  };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Function to format test result
            function formatTestResult(result) {
              const total = result.passed + result.failures + result.errors;
              const status = result.failures === 0 && result.errors === 0 ? '✅' : '❌';
              return { status, text: `${result.passed}/${total} passed` };
            }

            // Generate the summary content
            let summary = `## 🧪 Cross-Runtime Test Results\n\n`;
            summary += `This PR was tested across **3 JavaScript runtimes**:\n\n`;
            summary += `| Runtime | Status | Tests | Coverage |\n`;
            summary += `|---------|--------|-------|----------|\n`;

            // Read Deno test results
            let denoResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/deno-test-results.xml')) {
                const denoXml = fs.readFileSync('test-reports/deno-test-results.xml', 'utf8');
                console.log('Deno XML preview:', denoXml.substring(0, 500));
                const parsed = parseJUnit(denoXml);
                console.log('Deno parsed result:', parsed);
                denoResult = formatTestResult(parsed);
              } else {
                console.log('Deno test results file not found');
              }
            } catch (e) {
              console.log('Error reading Deno results:', e.message);
            }

            // Read Rust/Cargo test results
            let rustResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/cargo.xml')) {
                const cargoXml = fs.readFileSync('test-reports/cargo.xml', 'utf8');
                const parsed = parseJUnit(cargoXml);
                rustResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Rust results:', e.message);
            }

            // Read Bun test results
            let bunResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/bun-test-results.xml')) {
                const bunXml = fs.readFileSync('test-reports/bun-test-results.xml', 'utf8');
                const parsed = parseJUnit(bunXml);
                bunResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Bun results:', e.message);
            }

            // Read Node.js test results
            let nodeResult = { status: '❓', text: 'Not found' };
            try {
              if (fs.existsSync('test-reports/node-test-results.xml')) {
                const nodeXml = fs.readFileSync('test-reports/node-test-results.xml', 'utf8');
                const parsed = parseJUnit(nodeXml);
                nodeResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Node.js results:', e.message);
            }

            // Check for coverage files
            const denoCoverage = fs.existsSync('coverage-reports/deno-lcov.info') ? 'LCOV available' : 'No coverage';
            const rustCoverage = fs.existsSync('coverage-reports/rust.lcov') ? 'LCOV available' : 'No coverage';
            const bunCoverage = fs.existsSync('coverage-reports/bun-lcov.info') ? 'LCOV available' : 'No coverage';
            const nodeCoverage = fs.existsSync('coverage-reports/node-lcov.info') ? 'LCOV available' : 'No coverage';

            summary += `| 🦕 **Deno** | ${denoResult.status} | ${denoResult.text} | ${denoCoverage} |\n`;
            summary += `| 🦀 **Rust** | ${rustResult.status} | ${rustResult.text} | ${rustCoverage} |\n`;
            summary += `| 🥟 **Bun** | ${bunResult.status} | ${bunResult.text} | ${bunCoverage} |\n`;
            summary += `| 🟢 **Node.js** | ${nodeResult.status} | ${nodeResult.text} | ${nodeCoverage} |\n\n`;

            // List available artifacts
            summary += `### 📊 Test Artifacts Generated\n\n`;
            const testFiles = fs.existsSync('test-reports') ? fs.readdirSync('test-reports') : [];
            const coverageFiles = fs.existsSync('coverage-reports') ? fs.readdirSync('coverage-reports') : [];

            if (testFiles.length > 0) {
              summary += `**JUnit Reports**: ${testFiles.filter(f => f.endsWith('.xml')).join(', ')}\n\n`;
            }

            if (coverageFiles.length > 0) {
              summary += `**Coverage Reports**: ${coverageFiles.join(', ')}\n\n`;
            }

            // Overall status
            const overallSuccess = denoResult.status === '✅' && rustResult.status === '✅' && bunResult.status === '✅' && nodeResult.status === '✅';
            const overallStatus = overallSuccess ? '✅ All tests passing' : '❌ Some tests failed';
            summary += `**Overall Status**: ${overallStatus}\n\n`;

            // Add timestamp and run info
            const timestamp = new Date().toISOString().replace('T', ' ').substring(0, 19);
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            summary += `*Last updated: ${timestamp} UTC | [View CI Run](${runUrl})* 🤖`;

            // Look for existing comment from this bot
            const COMMENT_IDENTIFIER = '## 🧪 Cross-Runtime Test Results';

            try {
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              // Find existing test results comment
              const existingComment = comments.data.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes(COMMENT_IDENTIFIER)
              );

              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: summary
                });
                console.log('Updated existing test results comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created new test results comment');
              }
            } catch (error) {
              console.error('Error managing comment:', error);
              // Fallback: try to create a new comment
              try {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created fallback comment');
              } catch (fallbackError) {
                console.error('Fallback comment creation failed:', fallbackError);
              }
            }

      # Create status check based on test results
      - name: Create status check
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Function to parse JUnit XML and check if tests passed
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return { total: tests, errors, failures, passed: tests - errors - failures };
                }
              }

              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return { total: testcases, errors, failures, passed: testcases - errors - failures };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Check test results for each runtime
            const results = {};

            // Check Deno results
            if (fs.existsSync('test-reports/deno-test-results.xml')) {
              const xml = fs.readFileSync('test-reports/deno-test-results.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.deno = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            } else {
              results.deno = { success: false, total: 0, errors: 0, failures: 1, passed: 0 };
            }

            // Check Bun results
            if (fs.existsSync('test-reports/bun-test-results.xml')) {
              const xml = fs.readFileSync('test-reports/bun-test-results.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.bun = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            } else {
              results.bun = { success: false, total: 0, errors: 0, failures: 1, passed: 0 };
            }

            // Check Node.js results
            if (fs.existsSync('test-reports/node-test-results.xml')) {
              const xml = fs.readFileSync('test-reports/node-test-results.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.node = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            } else {
              results.node = { success: false, total: 0, errors: 0, failures: 1, passed: 0 };
            }

            // Rust tests are optional since there are no unit tests defined
            results.rust = { success: true, total: 0, errors: 0, failures: 0, passed: 0 };
            if (fs.existsSync('test-reports/cargo.xml')) {
              const xml = fs.readFileSync('test-reports/cargo.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.rust = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            }

            // Determine overall success
            const overallSuccess = results.deno.success && results.bun.success && results.node.success && results.rust.success;
            const totalTests = results.deno.total + results.bun.total + results.node.total + results.rust.total;
            const totalPassed = results.deno.passed + results.bun.passed + results.node.passed + results.rust.passed;
            const totalFailures = results.deno.failures + results.bun.failures + results.node.failures + results.rust.failures;
            const totalErrors = results.deno.errors + results.bun.errors + results.node.errors + results.rust.errors;

            console.log('Test Results Summary:');
            console.log(`- Deno: ${results.deno.passed}/${results.deno.total} passed (success: ${results.deno.success})`);
            console.log(`- Bun: ${results.bun.passed}/${results.bun.total} passed (success: ${results.bun.success})`);
            console.log(`- Node.js: ${results.node.passed}/${results.node.total} passed (success: ${results.node.success})`);
            console.log(`- Rust: ${results.rust.passed}/${results.rust.total} passed (success: ${results.rust.success})`);
            console.log(`- Overall: ${totalPassed}/${totalTests} passed (success: ${overallSuccess})`);

            const state = overallSuccess ? 'success' : 'failure';
            const description = overallSuccess
              ? `All tests passed (${totalPassed}/${totalTests})`
              : `Tests failed: ${totalFailures} failures, ${totalErrors} errors (${totalPassed}/${totalTests} passed)`;

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Cross-Runtime Tests'
              });
              console.log(`Status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create status check:', error);
              // If status check creation fails, fail the job so PR gates work
              if (!overallSuccess) {
                core.setFailed(`Tests failed: ${description}`);
              }
            }

            // Also fail the job if tests failed so the workflow shows failure
            if (!overallSuccess) {
              core.setFailed(`Cross-runtime tests failed: ${description}`);
            }

      # Create code quality status check
      - name: Create code quality status check
        if: ${{ always() }}
        uses: actions/github-script@v7
        with:
          script: |
            // Check code quality step results
            const steps = context.payload.workflow_run?.jobs?.[0]?.steps || [];

            // Get step outcomes from the build job
            const buildJob = context.payload.workflow_run?.jobs?.find(job => job.name === 'build');

            // Check each linting/formatting step result
            const rustFmt = '${{ steps.rust-fmt.outcome }}';
            const rustLint = '${{ steps.rust-lint.outcome }}';
            const denoFmt = '${{ steps.deno-fmt.outcome }}';
            const denoLint = '${{ steps.deno-lint.outcome }}';
            const eslint = '${{ steps.eslint.outcome }}';

            console.log('Code Quality Step Results:');
            console.log(`- Rust formatting: ${rustFmt}`);
            console.log(`- Rust linter (Clippy): ${rustLint}`);
            console.log(`- Deno formatting: ${denoFmt}`);
            console.log(`- Deno linter: ${denoLint}`);
            console.log(`- ESLint: ${eslint}`);

            // Count failures
            const checks = [
              { name: 'Rust formatting', result: rustFmt },
              { name: 'Rust linter', result: rustLint },
              { name: 'Deno formatting', result: denoFmt },
              { name: 'Deno linter', result: denoLint },
              { name: 'ESLint', result: eslint }
            ];

            const failed = checks.filter(check => check.result === 'failure');
            const passed = checks.filter(check => check.result === 'success');
            const skipped = checks.filter(check => check.result === 'skipped');

            const allPassed = failed.length === 0;
            const state = allPassed ? 'success' : 'failure';

            let description;
            if (allPassed) {
              description = `All code quality checks passed (${passed.length}/${checks.length})`;
            } else {
              const failedNames = failed.map(f => f.name).join(', ');
              description = `Code quality issues: ${failedNames} (${passed.length}/${checks.length} passed)`;
            }

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Code Quality'
              });
              console.log(`Code quality status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create code quality status check:', error);
            }
