name: Build

on:
  push:
    branches:
      - main
  pull_request_target:
    branches:
      - main

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    env:
      CARGO_TERM_COLOR: always
    permissions: {}
    outputs:
      rust-fmt-result: ${{ steps.rust-fmt.outcome }}
      rust-lint-result: ${{ steps.rust-lint.outcome }}
      deno-fmt-result: ${{ steps.deno-fmt.outcome }}
      deno-lint-result: ${{ steps.deno-lint.outcome }}
      eslint-result: ${{ steps.eslint.outcome }}
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      # Install system dependencies for Linux printing
      - name: Install CUPS development libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1

      # Cache Rust dependencies for faster builds
      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: ${{ runner.os }}-rust

      # Install cargo-llvm-cov for code coverage
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      # Install cargo2junit for JUnit XML output from Cargo tests
      - name: Install cargo2junit
        run: cargo install cargo2junit

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.
          cache: true

      # Setup Node.js for N-API module and testing
      - name: Setup Node.js
        uses: actions/setup-node@v5
        with:
          node-version: "20"
          cache: "npm"

      - name: Install NAPI-RS CLI
        run: npm install -g @napi-rs/cli

      # Setup Bun for cross-runtime testing
      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Install Task runner
      - name: Setup Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      # Code quality checks (continue on error to gather all results)
      - name: Check Rust formatting
        id: rust-fmt
        run: task fmt:rust:check
        continue-on-error: true

      - name: Run Rust linter (Clippy)
        id: rust-lint
        run: task lint:rust
        continue-on-error: true

      - name: Check Deno formatting
        id: deno-fmt
        run: task fmt:deno:check
        continue-on-error: true

      - name: Run Deno linter
        id: deno-lint
        run: task lint:deno
        continue-on-error: true

      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      - name: Run ESLint for non-Deno files
        id: eslint
        run: task lint:node
        continue-on-error: true

  build-and-test:
    name: Build and Test (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            name: Linux
          - os: macos-latest
            name: macOS
          - os: windows-latest
            name: Windows
    env:
      PRINTERS_JS_SIMULATE: true # Force simulation mode for all tests
      CARGO_TERM_COLOR: always
    permissions: {}
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Linux-specific setup
      - name: Disable man-db auto-update and install CUPS libraries (Linux)
        if: runner.os == 'Linux'
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1

      # Cache Rust dependencies for faster builds
      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: ${{ matrix.os }}-rust

      # Install cargo-llvm-cov for code coverage (Linux only)
      - name: Install cargo-llvm-cov (Linux only)
        if: runner.os == 'Linux'
        uses: taiki-e/install-action@cargo-llvm-cov

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      # Setup Bun
      - uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Setup Task runner
      - uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      # Setup Node.js
      - name: Setup Node
        uses: actions/setup-node@v5
        with:
          node-version: 20

      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      # Build all runtime libraries using our cross-platform TypeScript script
      - name: Build all runtime libraries
        run: task build

      # Run Cargo tests with coverage (Linux only)
      - name: Run Cargo tests with coverage (Linux only)
        if: runner.os == 'Linux'
        id: cargo-tests-coverage
        run: |
          mkdir -p test-results/coverage
          cargo llvm-cov --all-features --workspace --lcov --output-path test-results/coverage/rust.lcov test
        continue-on-error: true

      # Run Cargo tests (all platforms)
      - name: Run Cargo tests
        id: cargo-tests
        run: |
          mkdir -p test-results
          # Run cargo tests and capture output
          cargo test --all-features --workspace > cargo_test_output.txt 2>&1 || true

      # Generate JUnit report from Cargo test output
      - name: Generate Cargo test JUnit report
        id: cargo-junit
        run: |

          # Parse the test results from cargo output
          # Look for lines like "test result: ok. 16 passed; 0 failed; 0 ignored"
          RESULT_LINE=$(grep "test result:" cargo_test_output.txt | head -1)

          if [ -n "$RESULT_LINE" ]; then
            echo "Found test result line: $RESULT_LINE"

            # Extract numbers using more robust parsing
            PASSED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ passed" | grep -oE "[0-9]+" || echo "0")
            FAILED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ failed" | grep -oE "[0-9]+" || echo "0")
            IGNORED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ ignored" | grep -oE "[0-9]+" || echo "0")

            # Calculate total tests
            TOTAL=$((PASSED + FAILED + IGNORED))

            echo "Parsed: Total=$TOTAL, Passed=$PASSED, Failed=$FAILED, Ignored=$IGNORED"
          else
            echo "No test result line found, using defaults"
            TOTAL=0
            PASSED=0
            FAILED=0
            IGNORED=0
          fi

          # Generate JUnit XML
          echo '<?xml version="1.0" encoding="UTF-8"?>' > test-results/cargo.xml
          echo '<testsuites>' >> test-results/cargo.xml
          echo "  <testsuite name=\"Rust Cargo Tests\" tests=\"${TOTAL}\" failures=\"${FAILED}\" errors=\"0\" skipped=\"${IGNORED}\">" >> test-results/cargo.xml

          # Add individual test cases from the output
          grep "^test " cargo_test_output.txt | while IFS= read -r line; do
            # Extract test name and status
            TEST_NAME=$(echo "$line" | sed 's/test \(.*\) \.\.\. .*/\1/')
            TEST_STATUS=$(echo "$line" | sed 's/.*\.\.\. \(.*\)/\1/')

            if [ "$TEST_STATUS" = "ok" ]; then
              echo "    <testcase name=\"${TEST_NAME}\" classname=\"cargo\" />" >> test-results/cargo.xml
            elif [ "$TEST_STATUS" = "FAILED" ]; then
              echo "    <testcase name=\"${TEST_NAME}\" classname=\"cargo\">" >> test-results/cargo.xml
              echo "      <failure message=\"Test failed\" />" >> test-results/cargo.xml
              echo "    </testcase>" >> test-results/cargo.xml
            fi
          done

          echo '  </testsuite>' >> test-results/cargo.xml
          echo '</testsuites>' >> test-results/cargo.xml

          echo "Generated JUnit XML with $TOTAL tests"
          cat test-results/cargo.xml
        continue-on-error: true

      # Run comprehensive cross-runtime tests using our TypeScript script
      - name: Run cross-runtime tests
        id: cross-runtime-tests
        run: task test
        continue-on-error: true

      - name: Upload Test Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: test-reports-${{ matrix.name }}
          path: test-results/*.xml
          if-no-files-found: ignore

      - name: Upload Coverage Reports (Linux only)
        if: ${{ always() && runner.os == 'Linux' }}
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: test-results/coverage/*
          if-no-files-found: ignore

  report:
    if: ${{ always() && github.event_name == 'pull_request_target'}}
    runs-on: ubuntu-latest
    needs: [code-quality, build-and-test]
    permissions:
      contents: read
      pull-requests: write
      actions: read
      checks: write
      statuses: write
    steps:
      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true
        with:
          pattern: "*-reports"
          merge-multiple: true

      # Create combined coverage report from all runtimes
      - name: Combine coverage reports
        run: |
          # Coverage files are flattened in current directory after artifact download

          # Create coverage-reports directory for organized output
          mkdir -p coverage-reports

          # Create combined report from files in current directory
          if [ -f rust.lcov ]; then
            cp rust.lcov coverage-reports/rust.lcov
            cp rust.lcov coverage-reports/combined.lcov
          else
            touch coverage-reports/combined.lcov
          fi

          if [ -f deno-lcov.info ]; then
            cp deno-lcov.info coverage-reports/deno-lcov.info
            cat deno-lcov.info >> coverage-reports/combined.lcov
          fi

          # Copy other coverage files to organized directory
          if [ -f bun-lcov.info ]; then
            cp bun-lcov.info coverage-reports/bun-lcov.info
          fi

          if [ -f node-lcov.info ]; then
            cp node-lcov.info coverage-reports/node-lcov.info
          fi

          # Clean up the combined coverage file to remove problematic entries
          # This filters out TLA:GNC lines which can cause genhtml errors
          if [ -f coverage-reports/combined.lcov ] && [ -s coverage-reports/combined.lcov ]; then
            cd coverage-reports
            grep -v "TLA:GNC" combined.lcov > combined_clean.lcov || cp combined.lcov combined_clean.lcov
            mv combined_clean.lcov combined.lcov
            cd ..
          fi

      # Add/update dynamic test summary comment based on actual results
      - name: Update Test Summary Comment
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Function to parse JUnit XML (handles different formats)
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              // Pattern 1: Standard JUnit with tests, errors, failures attributes
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              // Pattern 2: Alternative format with different attribute order
              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return {
                    total: tests,
                    errors: errors,
                    failures: failures,
                    passed: tests - errors - failures
                  };
                }
              }

              // Pattern 3: Count individual testcase elements
              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return {
                    total: testcases,
                    errors: errors,
                    failures: failures,
                    passed: testcases - errors - failures
                  };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Function to format test result
            function formatTestResult(result) {
              const total = result.passed + result.failures + result.errors;
              const status = result.failures === 0 && result.errors === 0 ? '✅' : '❌';
              return { status, text: `${result.passed}/${total} passed` };
            }

            // Parse LCOV coverage file to get coverage percentage
            function parseLCOVCoverage(filePath) {
              try {
                if (!fs.existsSync(filePath)) {
                  return 'No coverage';
                }

                const lcovContent = fs.readFileSync(filePath, 'utf8');

                // Count total and hit lines
                let totalLines = 0;
                let hitLines = 0;

                const lines = lcovContent.split('\n');
                for (const line of lines) {
                  if (line.startsWith('DA:')) {
                    // DA:line_number,hit_count
                    totalLines++;
                    const hitCount = parseInt(line.split(',')[1] || '0');
                    if (hitCount > 0) {
                      hitLines++;
                    }
                  }
                }

                if (totalLines === 0) {
                  return 'No data';
                }

                const percentage = Math.round((hitLines / totalLines) * 100);
                return `${percentage}% (${hitLines}/${totalLines})`;
              } catch (error) {
                console.log('Error parsing LCOV:', error.message);
                return 'Parse error';
              }
            }

            // Generate the summary content
            let summary = `## 🧪 Test Results\n\n`;

            summary += `### JavaScript Runtime Tests\n`;
            summary += `Cross-runtime compatibility verified across **3 JavaScript runtimes**:\n\n`;
            summary += `| Runtime | Status | Tests | Coverage |\n`;
            summary += `|---------|--------|-------|----------|\n`;

            // Aggregate test results from matrix job platforms
            const platformResults = {};
            const results = {};

            // Debug: List all available files
            console.log('Available files in current directory:');
            const allFiles = fs.readdirSync('.');
            console.log('All files:', allFiles);
            console.log('XML files:', allFiles.filter(f => f.endsWith('.xml')));

            // Get outcomes from build-and-test matrix jobs
            const matrixResults = ${{ toJSON(needs.build-and-test.result) }};
            const matrixOutputs = ${{ toJSON(needs.build-and-test.outputs) }};
            console.log('Matrix job result:', matrixResults);
            console.log('Matrix job outputs:', matrixOutputs);

            // Initialize runtime results aggregation
            const runtimes = ['deno', 'bun', 'node', 'rust'];
            for (const runtime of runtimes) {
              results[runtime] = {
                aggregated: { success: false, total: 0, errors: 0, failures: 0, passed: 0 }
              };
            }

            // Look for test result files from all platforms
            const testFiles = allFiles.filter(f => f.endsWith('.xml'));
            console.log('Processing test files:', testFiles);

            // Process each test file
            for (const fileName of testFiles) {
              try {
                const xml = fs.readFileSync(fileName, 'utf8');
                const parsed = parseJUnit(xml);

                // Determine which runtime this file belongs to
                let runtime = null;
                if (fileName.includes('deno')) runtime = 'deno';
                else if (fileName.includes('bun')) runtime = 'bun';
                else if (fileName.includes('node')) runtime = 'node';
                else if (fileName.includes('cargo')) runtime = 'rust';

                if (runtime) {
                  // Aggregate results across all platforms for this runtime
                  const currentResults = results[runtime].aggregated;
                  currentResults.total += parsed.total;
                  currentResults.errors += parsed.errors;
                  currentResults.failures += parsed.failures;
                  currentResults.passed += parsed.passed;
                  currentResults.success = currentResults.failures === 0 && currentResults.errors === 0;

                  console.log(`Processed ${fileName} for ${runtime}: ${parsed.passed}/${parsed.total} passed`);
                }
              } catch (error) {
                console.log(`Error processing ${fileName}:`, error.message);
              }
            }

            // Fallback: if no results found, mark as failed
            for (const runtime of runtimes) {
              if (results[runtime].aggregated.total === 0) {
                results[runtime].aggregated.success = false;
                results[runtime].aggregated.failures = 1;
              }
            }

            // Format results for comment display
            const denoResult = formatTestResult(results.deno.aggregated);
            const bunResult = formatTestResult(results.bun.aggregated);
            const nodeResult = formatTestResult(results.node.aggregated);
            const rustResult = formatTestResult(results.rust.aggregated);

            // Debug: List available files
            console.log('Available files in current directory:');
            const files = fs.readdirSync('.').filter(f => f.includes('lcov') || f.includes('info'));
            console.log('Coverage-related files:', files);

            // Parse coverage files for actual percentages (flattened after artifact download)
            const denoCoverage = parseLCOVCoverage('deno-lcov.info');
            const rustCoverage = parseLCOVCoverage('rust.lcov');
            const bunCoverage = parseLCOVCoverage('bun-lcov.info');
            const nodeCoverage = parseLCOVCoverage('node-lcov.info');

            summary += `| 🦕 **Deno** | ${denoResult.status} | ${denoResult.text} | ${denoCoverage} |\n`;
            summary += `| 🥟 **Bun** | ${bunResult.status} | ${bunResult.text} | ${bunCoverage} |\n`;
            summary += `| 🟢 **Node.js** | ${nodeResult.status} | ${nodeResult.text} | ${nodeCoverage} |\n\n`;

            summary += `### Rust Library Tests\n`;
            summary += `Core library functionality tested in **Rust**:\n\n`;
            summary += `| Component | Status | Tests | Coverage |\n`;
            summary += `|-----------|--------|-------|----------|\n`;
            summary += `| 🦀 **Core Library** | ${rustResult.status} | ${rustResult.text} | ${rustCoverage} |\n\n`;

            // List available artifacts
            summary += `### 📊 Test Artifacts Generated\n\n`;
            const testFiles = fs.existsSync('test-reports') ? fs.readdirSync('test-reports') : [];
            const coverageFiles = fs.existsSync('coverage-reports') ? fs.readdirSync('coverage-reports') : [];

            if (testFiles.length > 0) {
              summary += `**JUnit Reports**: ${testFiles.filter(f => f.endsWith('.xml')).join(', ')}\n\n`;
            }

            if (coverageFiles.length > 0) {
              summary += `**Coverage Reports**: ${coverageFiles.join(', ')}\n\n`;
            }

            // Overall status based on matrix results and test parsing
            const matrixSuccess = matrixResults === 'success';
            const testResultsSuccess = results.deno.aggregated.success &&
                                    results.bun.aggregated.success &&
                                    results.node.aggregated.success &&
                                    results.rust.aggregated.success;
            const overallSuccess = matrixSuccess && testResultsSuccess;
            const overallStatus = overallSuccess ? '✅ All tests passing' : '❌ Some tests failed';

            // Add platform information if matrix failed
            let platformInfo = '';
            if (!matrixSuccess) {
              platformInfo = ' (Matrix job failed on one or more platforms: Linux, macOS, Windows)';
            }

            summary += `**Overall Status**: ${overallStatus}${platformInfo}\n\n`;

            // Add timestamp and run info
            const timestamp = new Date().toISOString().replace('T', ' ').substring(0, 19);
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            summary += `*Last updated: ${timestamp} UTC | [View CI Run](${runUrl})* 🤖`;

            // Look for existing comment from this bot
            const COMMENT_IDENTIFIER = '## 🧪 Test Results';

            try {
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              // Find existing test results comment
              const existingComment = comments.data.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes(COMMENT_IDENTIFIER)
              );

              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: summary
                });
                console.log('Updated existing test results comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created new test results comment');
              }
            } catch (error) {
              console.error('Error managing comment:', error);
              // Fallback: try to create a new comment
              try {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created fallback comment');
              } catch (fallbackError) {
                console.error('Fallback comment creation failed:', fallbackError);
              }
            }

      # Create status check based on test results
      - name: Create status check
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            // Function to parse JUnit XML and check if tests passed
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return { total: tests, errors, failures, passed: tests - errors - failures };
                }
              }

              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return { total: testcases, errors, failures, passed: testcases - errors - failures };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Aggregate test results from matrix job platforms
            const platformResults = {};
            const results = {};

            // Debug: List all available files
            console.log('Available files in current directory:');
            const allFiles = fs.readdirSync('.');
            console.log('All files:', allFiles);
            console.log('XML files:', allFiles.filter(f => f.endsWith('.xml')));

            // Get outcomes from build-and-test matrix jobs
            const matrixResults = ${{ toJSON(needs.build-and-test.result) }};
            const matrixOutputs = ${{ toJSON(needs.build-and-test.outputs) }};
            console.log('Matrix job result:', matrixResults);
            console.log('Matrix job outputs:', matrixOutputs);

            // Initialize platform tracking
            const platforms = ['Linux', 'macOS', 'Windows'];
            for (const platform of platforms) {
              platformResults[platform] = {
                overall: 'unknown',
                runtimes: {}
              };
            }

            // Process artifacts from each platform
            // The matrix job uploads artifacts with names like "test-reports-Linux", "test-reports-macOS", "test-reports-Windows"
            // After download-artifact with merge-multiple: true, all files are flattened to current directory

            // Initialize runtime results aggregation
            const runtimes = ['deno', 'bun', 'node', 'rust'];
            for (const runtime of runtimes) {
              results[runtime] = {
                platforms: {},
                aggregated: { success: false, total: 0, errors: 0, failures: 0, passed: 0 }
              };
            }

            // Look for test result files from all platforms
            // Since files are merged, we need to parse all available test files
            const testFiles = allFiles.filter(f => f.endsWith('.xml'));
            console.log('Processing test files:', testFiles);

            // Process each test file
            for (const fileName of testFiles) {
              try {
                const xml = fs.readFileSync(fileName, 'utf8');
                const parsed = parseJUnit(xml);

                // Determine which runtime and platform this file belongs to
                let runtime = null;
                if (fileName.includes('deno')) runtime = 'deno';
                else if (fileName.includes('bun')) runtime = 'bun';
                else if (fileName.includes('node')) runtime = 'node';
                else if (fileName.includes('cargo')) runtime = 'rust';

                if (runtime) {
                  // For now, aggregate all results since we can't determine platform from filename
                  // This is a limitation of the current artifact merging approach
                  const currentResults = results[runtime].aggregated;
                  currentResults.total += parsed.total;
                  currentResults.errors += parsed.errors;
                  currentResults.failures += parsed.failures;
                  currentResults.passed += parsed.passed;
                  currentResults.success = currentResults.failures === 0 && currentResults.errors === 0;

                  console.log(`Processed ${fileName} for ${runtime}: ${parsed.passed}/${parsed.total} passed`);
                }
              } catch (error) {
                console.log(`Error processing ${fileName}:`, error.message);
              }
            }

            // Fallback: if no results found, mark as failed
            for (const runtime of runtimes) {
              if (results[runtime].aggregated.total === 0) {
                results[runtime].aggregated.success = false;
                results[runtime].aggregated.failures = 1;
              }
            }

            console.log('Aggregated runtime results:');
            for (const runtime of runtimes) {
              const r = results[runtime].aggregated;
              console.log(`- ${runtime}: ${r.passed}/${r.total} passed (success: ${r.success})`);
            }

            // Determine overall success from matrix job results and test parsing
            const matrixSuccess = matrixResults === 'success';
            const testResultsSuccess = results.deno.aggregated.success &&
                                    results.bun.aggregated.success &&
                                    results.node.aggregated.success &&
                                    results.rust.aggregated.success;
            const overallSuccess = matrixSuccess && testResultsSuccess;

            const totalTests = results.deno.aggregated.total + results.bun.aggregated.total + results.node.aggregated.total + results.rust.aggregated.total;
            const totalPassed = results.deno.aggregated.passed + results.bun.aggregated.passed + results.node.aggregated.passed + results.rust.aggregated.passed;
            const totalFailures = results.deno.aggregated.failures + results.bun.aggregated.failures + results.node.aggregated.failures + results.rust.aggregated.failures;
            const totalErrors = results.deno.aggregated.errors + results.bun.aggregated.errors + results.node.aggregated.errors + results.rust.aggregated.errors;

            console.log('Final Test Results Summary:');
            console.log(`- Deno: ${results.deno.aggregated.passed}/${results.deno.aggregated.total} passed (success: ${results.deno.aggregated.success})`);
            console.log(`- Bun: ${results.bun.aggregated.passed}/${results.bun.aggregated.total} passed (success: ${results.bun.aggregated.success})`);
            console.log(`- Node.js: ${results.node.aggregated.passed}/${results.node.aggregated.total} passed (success: ${results.node.aggregated.success})`);
            console.log(`- Rust: ${results.rust.aggregated.passed}/${results.rust.aggregated.total} passed (success: ${results.rust.aggregated.success})`);
            console.log(`- Matrix job success: ${matrixSuccess}`);
            console.log(`- Overall: ${totalPassed}/${totalTests} passed (success: ${overallSuccess})`);

            const state = overallSuccess ? 'success' : 'failure';
            let description;
            if (overallSuccess) {
              description = `All tests passed (${totalPassed}/${totalTests})`;
            } else {
              const issues = [];
              if (!matrixSuccess) {
                issues.push('matrix build jobs failed');
              }
              if (!testResultsSuccess) {
                issues.push(`${totalFailures} failures, ${totalErrors} errors`);
              }
              description = `Tests failed: ${issues.join('; ')} (${totalPassed}/${totalTests} passed)`;
            }

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Cross-Runtime Tests'
              });
              console.log(`Status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create status check:', error);
              // If status check creation fails, fail the job so PR gates work
              if (!overallSuccess) {
                core.setFailed(`Tests failed: ${description}`);
              }
            }

            // Also fail the job if tests failed so the workflow shows failure
            if (!overallSuccess) {
              core.setFailed(`Cross-runtime tests failed: ${description}`);
            }

      # Create code quality status check
      - name: Create code quality status check
        if: ${{ always() }}
        uses: actions/github-script@v8
        with:
          script: |
            // Get individual step outcomes from build job outputs
            const rustFmtResult = '${{ needs.code-quality.outputs.rust-fmt-result }}';
            const rustLintResult = '${{ needs.code-quality.outputs.rust-lint-result }}';
            const denoFmtResult = '${{ needs.code-quality.outputs.deno-fmt-result }}';
            const denoLintResult = '${{ needs.code-quality.outputs.deno-lint-result }}';
            const eslintResult = '${{ needs.code-quality.outputs.eslint-result }}';

            console.log('Code quality step results:');
            console.log(`- Rust fmt: ${rustFmtResult}`);
            console.log(`- Rust lint: ${rustLintResult}`);
            console.log(`- Deno fmt: ${denoFmtResult}`);
            console.log(`- Deno lint: ${denoLintResult}`);
            console.log(`- ESLint: ${eslintResult}`);

            // Check if any code quality steps failed
            const codeQualityResults = {
              'Rust format': rustFmtResult,
              'Rust lint': rustLintResult,
              'Deno format': denoFmtResult,
              'Deno lint': denoLintResult,
              'ESLint': eslintResult
            };

            const failedChecks = Object.entries(codeQualityResults)
              .filter(([name, result]) => result === 'failure')
              .map(([name]) => name);

            const allPassed = failedChecks.length === 0;
            const state = allPassed ? 'success' : 'failure';

            let description;
            if (allPassed) {
              description = `All code quality checks passed (${Object.keys(codeQualityResults).length} checks)`;
            } else {
              description = `Code quality issues: ${failedChecks.join(', ')} failed`;
            }

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Code Quality'
              });
              console.log(`Code quality status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create code quality status check:', error);
            }
